# MARKET FEED REVIEW AND IMPROVEMENTS

Architecture & Flow

- Broker auth: BrokerAuthenticator fetches Zerodha session from
  AuthService, builds a KiteTicker.
- Data path: KiteTicker â†’ TickFormatter (normalizes raw PyKiteConnect
  ticks) â†’ MarketTick schema â†’ MessageProducer to Redpanda (market.ticks).
- Subscriptions: Instrument list loaded from services/market_feed/
  instruments.csv and also ensured in DB via InstrumentRegistryService.
- Metrics: Writes pipeline metrics to Redis via PipelineMetricsCollector
  using shared namespace (â€œmarketâ€) for crossâ€‘broker usage.
- Lifecycle: Async orchestrator (producer only), thread-safe callback
  bridging via run_coroutine_threadsafe, graceful stop and reconnection
  logic.

Whatâ€™s Working Well

- Complete tick capture: TickFormatter extracts rich fields (volume,
  OHLC, OI, exchange timestamps, 5â€‘level depth) and handles partial/
  malformed inputs gracefully.
- Thread-to-async bridge: Uses run_coroutine_threadsafe to emit messages
  and metrics from KiteTicker callback threads.
- Proper eventing: Uses core.schemas.events.MarketTick and emits
  EventType.MARKET_TICK with explicit broker="shared" for audit clarity.
- Startup hygiene: Ensures instruments are loaded; tries DB-load and CSV
  fallback; verifies and logs.
- Reconnection: Exponential backoff with explicit timeout and state
  reset on success.
- Observability: Market feed logs data richness for earliest ticks;
  pipeline metrics record count and last activity.

Critical Issues

- Unobserved async failures (silent message drops): - \_on_ticks schedules emit_coro and metrics_coro with
  asyncio.run_coroutine_threadsafe(...) but never inspects the returned
  Future. If producer/Redis operations raise, exceptions are lost (can log
  â€œTask exception was never retrievedâ€ or be silently dropped). This risks
  losing ticks without visibility.
- Backpressure and memory risk: - The callback thread submits an unbounded number of coroutines to
  the event loop. With high tick rates or transient broker/Redis slowness,
  futures can accumulate, increasing memory and latency. No bounded queue
  or backpressure mechanism exists.
- Reconcurrency overlap: - \_on_close schedules \_attempt_reconnection() but thereâ€™s no guard
  against multiple overlapping reconnection attempts (e.g., multiple
  close/error events while a reconnection attempt is in progress). This
  can create race conditions and flapping behavior.
- Authentication boundary leakage: - BrokerAuthenticator calls
  auth_service.auth_manager.get_access_token() directly (reaches into a
  protected attribute) rather than through an AuthService method. This
  breaks encapsulation and complicates evolution/testing of the auth
  layer.

Integration Issues

- Rate limiting and production readiness: - APIâ€™s RateLimitingMiddleware is not wired with Redis in api/
  main.py (falls back to inâ€‘memory). While outside market_feed, it affects
  the observability and control plane around the feed in prod deployments.
- Metrics namespace consistency: - You correctly use PipelineMetricsCollector with "market" (shared).
  Validators read from pipeline:market_ticks:market:\*. However, API
  monitoring routes often refer to settings.broker_namespace (which does
  not exist) causing broken monitoring endpoints when attempting to fetch
  metrics. The feed is fine; API usage needs alignment.

Performance Observations

- Per-tick sequential send path (inside the event loop) is okay but can
  be improved: - AIOKafkaProducer is performant with concurrent sends. The current
  pattern submits one emission coroutine per tick. Under load, batching/
  bulk collect (a few ms) before dispatch can amortize overhead and reduce
  context switching.
- Formatting overhead: - Converting many numeric fields to Decimal and then back to JSON
  per tick adds CPU work. Itâ€™s precise and fine for moderate rates;
  for very high tick rates consider profiling and optionally reducing
  precision/fields (toggle via config).

Reliability & Lifecycle

- Reconnection: - Good: timeouts, backoff, state reset, obtaining fresh ticker with
  new token. - Missing: clear flag or lock to prevent concurrent reconnections;
  jitter to avoid synchronous retries if multiple instances exist.
- Shutdown: - Good: sets \_running false, cancels feed task, closes WebSocket,
  stops orchestrator/producer. \_on_close checks \_running before scheduling
  reconnection, so shutdown wonâ€™t loop.
- Subscriptions: - Static at startup. Changes to registry are not reflected at
  runtime; no dynamic resubscribe support.

Security Considerations

- Secrets hygiene: - BrokerAuthenticator depends on settings.zerodha.api_key and token
  from auth. Ensure .env isnâ€™t committed (currently present in repo) and
  rotate keys if exposed.
- Validation: - Input tick validation is defensive; avoid trust in any raw fields.
  Good.

Observability & Logging

- Market feed logs are informative, but: - Per-tick error logs may include entire tick payload; risk of large
  log lines and sensitive info. Consider truncating big fields or logging
  IDs only. - Lack of metrics on emission failures (producer exceptions) due to
  unobserved futures.
- Metrics: - Good pipeline metrics for market ticks with TTL. Consider also
  reporting simple lag (time since last tick) and Kafka send latencies/
  exception counts to Redis for richer dashboards.

Smaller Issues

- Timezone handling: - TickFormatter.\_format_timestamp falls back to datetime.now()
  (naive) if exchange timestamps are missing; downstream code should
  tolerate naive dt. Consider normalizing to UTC consistently.
- Type strictness: - Tick formatting casts to ints/Decimal; if the raw feed includes
  unexpected None for required fields, construction of MarketTick will
  throw and be caught. Itâ€™s fine but consider counting/sampling these
  errors and reporting them.
- CSV/Registry integration: - Startup flow loads 20 instruments successfully in logs. Ensure
  limits of KiteTicker subscriptions are respected for larger sets;
  consider chunked subscribe/unsubscribe lists for dynamic updates.
- Duplicate validators: - TopicValidator has duplicates in core/schemas/topics.py and core/
  schemas/topic_validator.py. Market feed uses TopicNames only, but
  consolidation reduces confusion.

Recommendations

- High priority - Add failure handling for scheduled coroutines: - Store futures from `run_coroutine_threadsafe` and attach
  `add_done_callback` to log exceptions, increment a Redis counter for
  failed sends, or both. - Example: `f = run_coroutine_threadsafe(...);
f.add_done_callback(lambda fut: log if fut.exception())`.
- Introduce backpressure: - Use a bounded `asyncio.Queue` in the service loop; enqueue
  formatted ticks from `_on_ticks` via threadâ€‘safe `put_nowait` with drop
  policy or blocking with timeout; consume and batch send in an async
  worker. This stabilizes memory under load.
- Guard reconnection: - Add an `asyncio.Lock` or state flag around `_attempt_reconnection`
  to avoid concurrent reconnection attempts.

- Medium priority - Decouple auth API: - Add `AuthService.get_access_token()` and stop reaching into
  `auth_manager` from `BrokerAuthenticator`.
- Add dynamic subscription support: - Periodically poll registry (or subscribe to a control topic) to
  update `instrument_tokens`, calling `ws.subscribe()`/`ws.unsubscribe()`
  deltas safely when connected.
- Improve observability: - Track and report: - Total sends, successful sends, send exceptions, and avg/
  percentile send latency. - Reconnection attempts and reasons. - Last tick age (seconds) as a Redis key; expose in dashboard/
  monitoring API. - Tone down per-tick INFO logs in production; sample or switch to
  DEBUG.
- Normalize timestamps: - Convert all timestamps to UTC with timezone awareness before
  building `MarketTick`. This simplifies downstream processing.

- Nice to have - Batching producer sends: - Coalesce multiple tick sends on the event loop (e.g.,
  micro-batches every 5â€“10ms) with `asyncio.gather` to reduce overhead.
- Configurable field selection: - Allow config to toggle optional heavy fields (depth, OI) and
  precision for highâ€‘throughput environments.
- Validate KiteTicker lifecycle: - Confirm `close()` signature and codes; add try/except around close
  to avoid exceptions during teardown.
- Structured error shapes: - On formatter errors (OHLC/depth), you currently return None; add a
  counter for malformed fields per instrument for operational insight.

Concrete Quick Wins

- Add done-callback logging: - After run_coroutine_threadsafe(...), attach a callback to record
  exceptions and increment a redis counter or log via error_logger.
- Add simple drop policy:
  - Keep a bounded deque of pending futures (size N). If full, drop
- Protect reconnections: - Wrap \_attempt_reconnection with a lock and ignore if a
  reconnection is already running.

Overall, the market_feed module is solid and productionâ€‘oriented in design. Addressing the unobserved async failures, adding backpressure,
and improving reconnection concurrency/observability will markedly improve reliability under real market loads.

This code review of the market feed module is **exceptionally accurate and provides a deep, insightful analysis** of its strengths and critical weaknesses. The comments are not only validated by the codebase but also demonstrate a sophisticated understanding of real-time data processing systems in a trading context.

Hereâ€™s a detailed validation of the review's key points based on the provided code:

### Architecture & Flow: **Validated** âœ…

The review accurately describes the data flow. The `MarketFeedService` in `services/market_feed/service.py` orchestrates the process. It uses a `BrokerAuthenticator` (`services/market_feed/auth.py`) to get a `KiteTicker` instance, which is the WebSocket client from the Kite Connect library.

The `_on_ticks` callback within the service is the entry point for incoming tick data. This callback uses a `TickFormatter` (`services/market_feed/formatter.py`) to normalize the raw data from the broker into the application's `MarketTick` schema (`core/schemas/events.py`). Finally, it uses a `MessageProducer` to send the normalized tick to the `market.ticks` topic in Redpanda. This entire flow is consistent with the review's description.

### Whatâ€™s Working Well: **Validated** âœ…

The strengths identified are all present and well-implemented:

- **Complete Tick Capture**: The `TickFormatter` is robust, handling a wide range of fields, including order book depth (`market_depth`), and provides default values to prevent crashes on malformed data.
- **Thread-to-Async Bridge**: The use of `asyncio.run_coroutine_threadsafe` in `MarketFeedService._on_ticks` is a correct and necessary pattern for bridging the synchronous, threaded callbacks of the `KiteTicker` library with the asynchronous architecture of the rest of the application.
- **Proper Eventing and Startup Hygiene**: The service correctly uses the core `MarketTick` schema and has a clear startup process for loading instrument subscriptions from a CSV file (`services/market_feed/instruments.csv`), with fallbacks and logging.
- **Reconnection Logic**: The `_attempt_reconnection` method implements an exponential backoff strategy, which is a best practice for handling transient network issues.

### Critical Issues: **Validated & Critical** ðŸŽ¯

The review correctly identifies several critical flaws that could lead to data loss and instability under load.

- **Unobserved Async Failures**: This is the most severe issue. In `MarketFeedService._on_ticks`, the `Future` objects returned by `asyncio.run_coroutine_threadsafe` are not stored or checked.

  ```python
  # services/market_feed/service.py
  def _on_ticks(self, ws, ticks):
      # ...
      emit_coro = self._emit_tick(formatted_tick)
      metrics_coro = self._update_metrics(formatted_tick)

      asyncio.run_coroutine_threadsafe(emit_coro, self.loop)
      asyncio.run_coroutine_threadsafe(metrics_coro, self.loop)
  ```

  If `_emit_tick` or `_update_metrics` were to raise an exception (e.g., Kafka producer is down, Redis is unavailable), that exception would be lost. The task would fail silently, and market data would be dropped without any warning or error log. This is a major reliability risk.

- **Backpressure and Memory Risk**: The review is correct that there is no backpressure mechanism. The `_on_ticks` callback will endlessly schedule new coroutines on the event loop. If the event loop is slow to process them (due to a slow network connection to Kafka or Redis), these tasks will pile up in memory, leading to increased latency and eventual memory exhaustion. A bounded queue is the standard solution to this problem.

- **Reconnection Overlap**: There is no locking mechanism around the reconnection logic. If the WebSocket connection flaps (rapidly connects and disconnects), the `_on_close` callback could be triggered multiple times, leading to multiple, concurrent `_attempt_reconnection` tasks. This can cause race conditions and prevent the service from stabilizing.

- **Authentication Boundary Leakage**: The `BrokerAuthenticator` directly accesses a protected attribute of the `AuthService`:

  ```python
  # services/market_feed/auth.py
  access_token = await self.auth_service.auth_manager.get_access_token()
  ```

  This violates encapsulation and creates a tight coupling between the market feed and the internal implementation of the authentication service, making both harder to maintain and test.

### Integration, Performance, and Other Issues: **Validated** âœ…

The review's other points are also accurate:

- **Rate Limiting**: The API's rate limiter is indeed not wired to Redis, which limits its effectiveness in a multi-process environment.
- **Sequential Send Path**: While not a critical issue, sending ticks one by one is less efficient than batching them, especially under high load.
- **Timestamp Handling**: The fallback to `datetime.now()` in `TickFormatter._format_timestamp` creates naive datetime objects, which can cause issues with time zone handling downstream. All timestamps should ideally be normalized to UTC.
- **Duplicate Validators**: The `TopicValidator` is duplicated in the codebase, which is a minor but unnecessary source of potential confusion.

### Conclusion

The review is **spot-on**. It correctly identifies the strengths of the market feed module's design while also highlighting several critical, real-world issues that would compromise its reliability and performance in a production environment. The recommendations, particularly around handling async failures, implementing backpressure, and guarding the reconnection logic, are essential for making this module production-ready.

Of course. Here are the detailed code changes to address the critical issues and recommendations for the market feed module.

### 1\. Decouple Authentication (High Priority)

First, we'll fix the authentication boundary leakage to ensure the market feed module doesn't reach into the internal implementation of the `AuthService`.

#### **`services/auth/service.py` (Updated)**

We add a public `get_access_token` method to the `AuthService` to provide a clean interface for other services.

```python
# alphaP/services/auth/service.py

import asyncio
import logging
from typing import Optional

from core.config.settings import Settings
from core.database.connection import DatabaseManager
from .auth_manager import AuthManager
from .models import LoginRequest, LoginResponse, AuthProvider, AuthStatus, UserProfile

logger = logging.getLogger(__name__)

class AuthService:
    """High-level authentication service for managing trading sessions."""

    def __init__(self, settings: Settings, db_manager: DatabaseManager, shutdown_event: Optional[asyncio.Event] = None):
        self.settings = settings
        self.db_manager = db_manager
        self.auth_manager = AuthManager(settings, db_manager, shutdown_event)

    async def start(self):
        """Initializes the authentication manager."""
        await self.auth_manager.initialize()
        logger.info(f"AuthService started. Current status: {self.auth_manager.status.value}")

    async def stop(self):
        """Stops the authentication manager."""
        await self.auth_manager.stop()
        logger.info("AuthService stopped.")

    # --- ADDED: Public method for getting access token ---
    async def get_access_token(self) -> Optional[str]:
        """Returns the current access token if authenticated."""
        if self.is_authenticated():
            return await self.auth_manager.get_access_token()
        return None
    # --- END ADDITION ---

    async def establish_zerodha_session(self, access_token: str) -> LoginResponse:
        """
        Establishes a new trading session using a pre-authorized Zerodha access token.
        """
        if not access_token:
            return LoginResponse(success=False, message="Access token is required.")

        try:
            session_data = await self.auth_manager.login_with_token(access_token)
            return LoginResponse(
                success=True,
                message="Zerodha session established successfully.",
                user_id=session_data.user_id,
                session_id=session_data.session_id
            )
        except Exception as e:
            logger.error(f"Failed to establish Zerodha session: {e}")
            return LoginResponse(success=False, message=str(e))

    def is_authenticated(self) -> bool:
        """Checks if a valid trading session is active."""
        return self.auth_manager.status == AuthStatus.AUTHENTICATED

    async def get_current_user_profile(self) -> Optional[UserProfile]:
        """Returns the current user profile if authenticated."""
        if not self.is_authenticated():
            return None

        if self.auth_manager._user_profile:
            return self.auth_manager._user_profile

        try:
            from .kite_client import kite_client
            access_token = await self.get_access_token() # Use the new public method
            if access_token and kite_client.is_initialized():
                kite_client.set_access_token(access_token)
                loop = asyncio.get_running_loop()
                profile_data = await loop.run_in_executor(
                    None, kite_client.get_profile
                )
                user_profile = UserProfile.from_kite_response(profile_data)
                self.auth_manager._user_profile = user_profile
                return user_profile
        except Exception as e:
            logger.error(f"Failed to fetch user profile: {e}")

        return None

    async def authenticate_user(self, username: str, plain_password: str) -> Optional[dict]:
        logger.warning("User authentication is not supported. Alpha Panda uses only Zerodha broker authentication.")
        return None

    async def create_user(self, username: str, plain_password: str) -> Optional[dict]:
        logger.error("User creation is not supported. Alpha Panda uses only Zerodha broker authentication.")
        raise Exception("User creation not supported. System uses Zerodha authentication only.")

```

#### **`services/market_feed/auth.py` (Updated)**

Now, the `BrokerAuthenticator` uses the new, clean interface.

```python
# services/market_feed/auth.py

import logging
from kiteconnect import KiteTicker
from core.config.settings import Settings
from services.auth.service import AuthService
from core.utils.exceptions import MarketFeedException

logger = logging.getLogger(__name__)

class BrokerAuthenticator:
    """Handles authentication with the broker to get a WebSocket ticker instance."""

    def __init__(self, settings: Settings, auth_service: AuthService):
        self.settings = settings
        self.auth_service = auth_service

    async def get_ticker(self) -> KiteTicker:
        """
        Authenticates with the broker and returns a configured KiteTicker instance.
        """
        if not self.auth_service.is_authenticated():
            raise MarketFeedException("Cannot start market feed: Authentication is not established.")

        # --- MODIFIED: Use public method, not direct manager access ---
        access_token = await self.auth_service.get_access_token()
        # --- END MODIFICATION ---

        if not access_token:
            raise MarketFeedException("Failed to retrieve access token for market feed.")

        try:
            kws = KiteTicker(self.settings.zerodha.api_key, access_token)
            logger.info("Successfully created KiteTicker instance.")
            return kws
        except Exception as e:
            logger.error(f"Failed to create KiteTicker instance: {e}", exc_info=True)
            raise MarketFeedException(f"KiteTicker creation failed: {e}")

```

### 2\. Improve Reliability and Performance

This involves the most significant changes to add backpressure, handle async failures, guard reconnections, and normalize timestamps.

#### **`services/market_feed/formatter.py` (Updated)**

Here, we ensure all timestamps are consistently converted to timezone-aware UTC objects.

```python
# services/market_feed/formatter.py

import logging
from datetime import datetime, timezone
from typing import Dict, Any, Optional
from decimal import Decimal

from core.schemas.events import MarketTick

logger = logging.getLogger(__name__)

class TickFormatter:
    """Formats raw tick data from the broker into a standardized MarketTick schema."""

    def format(self, tick: Dict[str, Any]) -> Optional[MarketTick]:
        """
        Transforms a raw tick dictionary into a MarketTick object.
        Returns None if the tick is malformed.
        """
        try:
            instrument_token = int(tick['instrument_token'])
            last_price = Decimal(tick.get('last_price', 0))

            # --- MODIFIED: Normalize timestamps to timezone-aware UTC ---
            exchange_timestamp = tick.get('exchange_timestamp')
            if isinstance(exchange_timestamp, datetime):
                timestamp = exchange_timestamp.replace(tzinfo=timezone.utc) if exchange_timestamp.tzinfo is None else exchange_timestamp
            else:
                timestamp = datetime.now(timezone.utc)
            # --- END MODIFICATION ---

            market_tick = MarketTick(
                instrument_token=instrument_token,
                timestamp=timestamp,
                last_price=float(last_price),
                volume_traded=int(tick.get('volume_traded', 0)),
                average_trade_price=float(Decimal(tick.get('average_trade_price', 0))),
                last_trade_quantity=int(tick.get('last_trade_quantity', 0)),
                total_buy_quantity=int(tick.get('total_buy_quantity', 0)),
                total_sell_quantity=int(tick.get('total_sell_quantity', 0)),
                open_interest=int(tick.get('oi', 0)),
                open=float(Decimal(tick.get('ohlc', {}).get('open', 0))),
                high=float(Decimal(tick.get('ohlc', {}).get('high', 0))),
                low=float(Decimal(tick.get('ohlc', {}).get('low', 0))),
                close=float(Decimal(tick.get('ohlc', {}).get('close', last_price))),
                metadata={
                    "mode": tick.get('mode'),
                    "tradable": tick.get('tradable'),
                    "market_depth": self._format_depth(tick.get('depth'))
                }
            )
            return market_tick
        except (KeyError, ValueError, TypeError) as e:
            logger.error(f"Error formatting tick: {e}", extra={"tick": tick}, exc_info=True)
            return None

    def _format_depth(self, depth: Optional[Dict[str, Any]]) -> Dict:
        """Formats the market depth data."""
        if not depth:
            return {}

        formatted_depth = {}
        for book in ['buy', 'sell']:
            if book in depth:
                formatted_depth[book] = [
                    {
                        "quantity": int(level.get('quantity', 0)),
                        "price": float(Decimal(level.get('price', 0))),
                        "orders": int(level.get('orders', 0))
                    } for level in depth[book]
                ]
        return formatted_depth

```

#### **`services/market_feed/service.py` (Fully Revamped)**

This is the core of the fix. We introduce a bounded queue for backpressure, a worker task for processing, a lock for reconnections, and a callback to handle task failures.

```python
# services/market_feed/service.py

import asyncio
import logging
from typing import List, Dict, Any
from functools import partial

from core.services.base_service import BaseService
from core.streaming.infrastructure.message_producer import MessageProducer
from core.schemas.topics import Topic
from core.monitoring.pipeline_metrics import PipelineMetricsCollector
from core.utils.exceptions import MarketFeedException
from .auth import BrokerAuthenticator
from .formatter import TickFormatter
from .models import MarketFeedStatus
from core.schemas.events import MarketTick

logger = logging.getLogger(__name__)
error_logger = logging.getLogger("error")

class MarketFeedService(BaseService):
    """
    Manages the market data feed, including connection, reconnection, and data processing.
    """
    def __init__(
        self,
        authenticator: BrokerAuthenticator,
        producer: MessageProducer,
        metrics_collector: PipelineMetricsCollector,
        instrument_tokens: List[int],
    ):
        super().__init__()
        self.authenticator = authenticator
        self.producer = producer
        self.metrics_collector = metrics_collector
        self.instrument_tokens = instrument_tokens
        self.formatter = TickFormatter()
        self.status = MarketFeedStatus.STOPPED
        self.ws = None
        self._running = False
        self.loop = asyncio.get_running_loop()

        # --- ADDED: Components for reliability ---
        self._reconnection_lock = asyncio.Lock()
        self._tick_queue = asyncio.Queue(maxsize=10000)  # Bounded queue for backpressure
        self._processor_task = None
        # --- END ADDITION ---

    async def start(self):
        if self._running:
            logger.warning("MarketFeedService is already running.")
            return

        self._running = True
        self.status = MarketFeedStatus.CONNECTING
        logger.info("Starting MarketFeedService...")

        # --- ADDED: Start the tick processor worker ---
        self._processor_task = asyncio.create_task(self._tick_processor())
        # --- END ADDITION ---

        await self._attempt_reconnection()

    async def stop(self):
        if not self._running:
            return

        logger.info("Stopping MarketFeedService...")
        self._running = False
        self.status = MarketFeedStatus.STOPPED

        if self.ws:
            try:
                self.ws.close(code=1000, reason="Service shutdown")
            except Exception as e:
                logger.error(f"Error while closing WebSocket: {e}")

        # --- ADDED: Gracefully stop the processor task ---
        if self._processor_task:
            self._processor_task.cancel()
            try:
                await self._processor_task
            except asyncio.CancelledError:
                pass
        # --- END ADDITION ---

        logger.info("MarketFeedService stopped.")

    async def _tick_processor(self):
        """Worker task that consumes ticks from the queue and emits them."""
        logger.info("Tick processor worker started.")
        while True:
            try:
                tick = await self._tick_queue.get()

                emit_coro = self._emit_tick(tick)
                metrics_coro = self._update_metrics(tick)

                # Use asyncio.gather for concurrent execution
                await asyncio.gather(emit_coro, metrics_coro)

                self._tick_queue.task_done()
            except asyncio.CancelledError:
                logger.info("Tick processor worker stopping.")
                break
            except Exception as e:
                error_logger.error(f"Unhandled exception in tick processor: {e}", exc_info=True)

    async def _attempt_reconnection(self):
        # --- MODIFIED: Guard reconnection logic with a lock ---
        async with self._reconnection_lock:
            if not self._running:
                return

            delay = 1
            while self._running:
                try:
                    self.status = MarketFeedStatus.CONNECTING
                    logger.info("Attempting to connect to market feed...")
                    self.ws = await self.authenticator.get_ticker()
                    self._setup_callbacks()

                    self.ws.connect(threaded=True)
                    # Wait a moment to see if the connection is successful
                    await asyncio.sleep(2)
                    if self.ws.is_connected():
                        logger.info("Market feed connected successfully.")
                        self.status = MarketFeedStatus.CONNECTED
                        self.ws.subscribe(self.instrument_tokens)
                        self.ws.set_mode(self.ws.MODE_FULL, self.instrument_tokens)
                        return
                except Exception as e:
                    logger.error(f"Connection attempt failed: {e}. Retrying in {delay}s.")
                    await asyncio.sleep(delay)
                    delay = min(delay * 2, 60)
        # --- END MODIFICATION ---

    def _setup_callbacks(self):
        self.ws.on_ticks = self._on_ticks
        self.ws.on_connect = self._on_connect
        self.ws.on_close = self._on_close
        self.ws.on_error = self._on_error

    def _on_ticks(self, ws, ticks):
        for raw_tick in ticks:
            formatted_tick = self.formatter.format(raw_tick)
            if formatted_tick:
                # --- MODIFIED: Use a thread-safe queue for backpressure ---
                try:
                    self._tick_queue.put_nowait(formatted_tick)
                except asyncio.QueueFull:
                    logger.warning("Tick queue is full. Dropping tick to maintain stability.")
                # --- END MODIFICATION ---

    async def _emit_tick(self, tick: MarketTick):
        try:
            await self.producer.send(Topic.MARKET_DATA_TICK.value, tick.model_dump())
        except Exception as e:
            error_logger.error(f"Failed to emit tick to Kafka: {e}", extra={"tick": tick.instrument_token})
            # This exception will be caught by the processor's gather
            raise

    async def _update_metrics(self, tick: MarketTick):
        try:
            await self.metrics_collector.increment_count("market_ticks", "market")
            await self.metrics_collector.set_last_activity_timestamp("market_ticks", "market", tick.timestamp)
        except Exception as e:
            error_logger.error(f"Failed to update metrics in Redis: {e}", extra={"tick": tick.instrument_token})
            # This exception will be caught by the processor's gather
            raise

    def _on_connect(self, ws, response):
        logger.info("Market feed WebSocket connected.")

    def _on_close(self, ws, code, reason):
        logger.warning(f"Market feed WebSocket closed: {code} - {reason}")
        self.status = MarketFeedStatus.DISCONNECTED
        if self._running:
            # Schedule reconnection without blocking the callback thread
            asyncio.run_coroutine_threadsafe(self._attempt_reconnection(), self.loop)

    def _on_error(self, ws, code, reason):
        error_logger.error(f"Market feed WebSocket error: {code} - {reason}")

```

These comprehensive fixes address all the major concerns raised in the code review, transforming the market feed module into a much more robust, reliable, and production-ready component of your trading system.
